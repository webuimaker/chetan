[{"categories":null,"contents":" Designing of research study should be given more attention than on data collection and analysis. It is difficult to re-do the poorly designed study but you can always re-analyse data with different method. Therefore, it cannot be stressed enough that best time to contact ISCON statistician is before designing your research study.\n The studies with poor planning i.e. vague research questions, unclear hypothesis, inadequate sample size, inappropriate choice of variables, improper randomisation, poor follow up are deemed to produce inaccurate and meaningless results. Statistician at ISCON Statistics can be your best companion help every steps of your research project – starting from research idea to development of study protocol to successful completion and reporting in high-impact journals. Our statistician can help you with-\n Formulating research questions and research hypothesis and develop statistical analysis plan Defining and clarifying the study outcomes- primary outcome, secondary outcome, data collection methods and frequency of the measurements Advice you about study design whether it’s appropriate to answer research questions or hypothesis Discuss with you the impact of bias, randomisation, allocation and blinding on results Discuss with you the required sample size (so study has optimum statistical power) in different conditions/hypothesis Help you to develop study protocol, statistical analysis plan, any secondary or interim analysis.  Help translating your research idea into a statistically answerable question In planning research study, it very important to formulate research question that is clear, specific and well defined. Because, it is your question that will determine the conduct of the study and method of statistical analysis. It is your research question that will guide the development of the study protocol, sample size estimation and power calculation. Well defined research questions have three essential characteristics. First, the research question should describe or hypothesize the relationship between two or more variables. Second, it should take the form of question and third, the variables in proposed questions must allowed to be observed or measured through either observation or by experiments. A vague research question not only leads to complexity of both data collection and subsequent statistical analyses but also indirectly affects reliability of the proposed study. The specific and clear research questions help statistician determining the sample size, power and method of the analysis - producing valid and meaningful results of your study. ISCON Statistics will. ISCON recommends PICO format to develop specific research question: Population of interest, the Intervention being studied, the Comparison group (or to what is the intervention being compared) and the Outcome of interest.\nLet us help you to derive clear research question from your research topic so that your research project conduct smoothly- drawing meaningful and accurate conclusion.\nHelp in articulating specific research hypothesis that drives your statistical analysis Your research question helps you to articulate specific research hypotheses. Hypothesis is basically a statement – a educated guess about potential relationship between two or more variables. Formulating a clear research hypothesis based on specific research question drives the smooth conduct of the study ultimately leads to valid data analysis. Only after research hypothesis being formulated, data will be collected and then tested statistically whether to reject or not to reject hypothesis, based on observed sample data. The research question and hypothesis need to be formulated before the start of the study, not after data being collected. It a research questions that should determine the hypothesis, not the observed data. Retrospectively formulating hypothesis from observed data carries risk of multiple statistical testing which than leads to declaring dubious statistical association.\nIn statistics, hypothesis can be null and alternative hypothesis. The null hypothesis states that there is no association, difference or correlation between two or more variable whereas alternative hypothesis states that there is association, difference or correlation. The statistical test or model than test this hypothesis whether to reject of the hypothesis. Hypothesis can be directional (one sided) or non-directional (two sided). In directional hypothesis, one has to specify not only the association but also direction of association-difference or correlation (how much greater, lesser or better or worse) in hypothesis statement.\nFrom your research idea, we help you to set clear research questions and formulate testable research hypothesis.\nHelping you choosing right variables for your research Variable is a characteristic that varies from person by person or item by item, i.e. takes different value in different person, item or thing. There are thousands of variables can be measured and reported in research literature, the question is which variables to choose and which to discard, if chosen too many, how it would be included in statistical analysis.\nVariables can be dependent or independent. The dependent variable is the variable which values are dependent on other variables- which are called independent variables. We are interested in assessing the effects of the independent variables on dependent variable (outcome). The better nomenclature would be - response variable and explanatory variables. We are interested is how explanatory variables (independent variables) explains the variation in response variable i.e. outcome (dependent variable).\nThe choice and number of variables in your study depends on multiple factors. It depends on your study objective- whether you are interested in specific phenomenon (limited numbers) or identifying important variables from collecting large number of variables (through specialized statistical methods). It also dependent on your subject specific knowledge – include variables that are important in a way that are associated with outcome and/or explanatory variables or both (known as confounding variable). Omitting or not handling confounding variables appropriately in statistical analysis often leads to biased and inaccurate conclusion. ISCON Statistician can help you to choose optimum number of variables and analyse your study with best possible methods getting you valid and unbiased statistical results.\nHelp in calculating sample size required in your research study so that your study has optimum statistical power Sample size is number of participants needed in the study to answer research questions. The aim of sample size determination is to calculate number of samples required in study in order to detect important, meaningful effect. We needed to calculate this number with great precision because if the sample size is too small, we many not be able to detect an important effect, whereas unnecessarily large sample size leads to waste of resources. So, it is so vital to optimize the sample size during planning phase as you want to make sure that effort you are making in research study is worthwhile i.e. should leads to valid and meaningful conclusion. It is also ethically undesirable to expose more participants than needed from side effect, toxicity of drug or intervention. Therefore, majority of ethics committee always ask about the optimum sample size in your research study before they give you approval.\nIn order to determine correct the sample size, you need to have clear research question, set of clear study hypothesis, details about your outcome variable, its variability and expected/predicted change in your outcome in your study. Specifically, you need to have details about four main items (1) the extent at which you want to have false positive results in your study (alpha which type 1 error rate, most commonly 5%) (2) probability that you want to detect the effect when it really exist (statistical power, commonly 80%) (3) smallest difference that clinically meaningful i.e. effect that you don’t want to miss, also known as minimal clinically significant difference(MCSD) and last (4) variability of outcome in population in which you going get sample (which you can gather from previously published studies).\nThe formula and method of sample size determination dependent many factors such as - study objectives, number of hypothesis, type of outcome (continuous, binary, time to event, count), number of group or explanatory variables in your study, type of your study design (observational or experimental study, parallel group clinical trial, cross-over trial, cluster or adaptive trials), method of statistical analysis (simple statistical test or advanced statistical models with covariates included such as liner mixed model with clustering) and resources (funding source, recruitment rate, staff availability, timelines).\nIts is prudent to ask statistician to calculate optimum sample size because the complexity involves in methods and scenarios and secondly, statistician may have expert knowledge of other efficient, specialized method (such as Monte Carlo simulation) than formulas which can establish optimum number of samples robustly. ISCON Statisticians particularly experienced in calculating and explaining the sample size calculation in different scenarios in wide variety of study designs and statistical methods. Get in touch with us for more details.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/research-design-methodology/","tags":null,"title":"Research Design and Methodology"},{"categories":null,"contents":"Transform Your Data into Comprehensible and Intelligent Insights with Data Visualisation  The descriptive statistics and statistical analysis are essential to summarise the relationship in the data, but it is also equally important to present the findings visually and make them comprehensible for a non-technical audience. Data visualisation helps decision-makers understand their data quickly and effectively without digging into complext statistical analysis.\n Data visualisation is a graphical tool that presents complex numerical information in a visual form that enhances our understating of the data and highlights trends, patterns, and relationships. It is a way to present large, complex data in a clear, accessible manner in order to develop better insight. Effective data visualisation tells a story, conveys the right message in the best and simplest form.\nHow ISCON statistician can help you to create precise and efficient visualisation so that it has the right impact We have been offering top-notch data visualization services for researchers and businesses. We think that data visualization is a key part of any data analytics. This is because it allows you to immediately spot any trends, easily identify outliers, track goal achievement, and compare the performance of various categories, brands, and products, etc. At ISCON Statistics, we understand the goals of data visualisation. We leverage our expertise and advanced data visualisation software (such as Plotly, GGplot2 in R, D3.js ) to present big data in an easily understandable and visually appealing format. We ensure precise and efficient representation of data so that it has the right impact. We can transform data into compelling experiences, meaningful stories, powerful and valuable insights, and unique tools. We are also proficient in interactive data visualisation. Interactive data visualisation empowers users who do not have knowledge of advanced statistics. These users can explore, manipulate, and interact with graphics solely by simply clicking a mouse button. Users can also change the variables, type of graph, and colour of graphics, etc. allowing for in-depth exploration.\nCharacteristics of effective data visualisation Effective data visualisation helps uncover trends and patterns from the data without distorting the data. Effective data visualisation also communicates findings clearly and effectively, highlighting important patterns and relationships. It also helps the audience understand difficult concepts easily and quickly. Edward Tufte, a prominent statistician, explained the characteristics of “ideal” data visualisation in his book “The Visual Display of Quantitative Information.” According to Tufte, data visualisation should have a clear purpose, should reveal several layers of details without distorting the facts, and should closely align with the statistical findings. Our skilled experts and specialists leverage their knowledge and experience to combine creative analytics with amazing reporting tools in order to develop and maintain robust relationships.\nInteractive data visualisations Interactive data visualisations allow users to change the components of graphics interchangeably on screen, allowing a greater understanding of the data. The usage of interactive data visualisations is now increasing because of ease of use and interactivity with real-time data. Interactive data visualisations also offer real-time access to data sources, making it invaluable in creating interactive dashboard- where information from a variety of sources are gathered and visually available in one screen.\nISCON statisticians are experienced in creating interactive dashboards and graphics using a variety of platforms, such as Plotly, R shiny, and ggplot2. Contact us now to develop interactive data visualisation or dashboard at an affordable cost.\nQuality figures for research publication Publishing your research in scientific journals requires that you have high-quality figures. It is important to derive figures with utmost care and diligence because these figures- being visual elements - are the only medium to convey your research findings clearly and most appealing to the scientific community.\nReliable figures bridge the gap between human cognition and numbers. They should be free from “chart-junk” (the word coined by Edward Tufte), which are unnecessary and confusing elements in figure, such as labels, gridlines, and background. It should have a detailed caption, explaining more details about the figure in a similar way you explain the figure to your audience while giving a presentation. It also has efficient use of colour, and highlights an important message.\nBenefits of our data visualisation services Improved data insights By taking our data visualisation services, you can significantly improve data insights, which is not possible with traditional descriptive statistical methods.\nConcise and effective data visualisation Our experienced professionals know how to develop data visuals that give you a complete picture of your data. Through our interactive visualisations, you can correlate, decipher, and connect critical data information to make informed decisions.\nMultiplatform compatibility We understand that you might need to access your data visuals through multiple platforms. Therefore, we create data visualisations that remain effective across multiple platforms without losing relevance.\nCustomizable services As an experienced statistical analysis services provider, we offer customizable data visualisation services based on your specific requirements.\nTransform charts and spreadsheets If your data is present in traditional formats like charts and spreadsheets, our experts can convert it into easily understandable and visually pleasing visuals. This not only infuses life into your complex data but also enhances its meaningfulness.\nThere are multiple ways you can convey your research findings accurately in the form of figures in scientific publications. Let ISCON Statistics help you produce figures that are clear, accurate, and convey all the relevant information in a meaningful way. ","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/data-visualisation/","tags":null,"title":"Data Visualisation"},{"categories":null,"contents":"What is statistics? Statistics is not only about collecting data and creating graphs and tables; it is a science of making inferences or predictions from observed data applied to a population. Precisely, statistics is a branch of science that deals with the collection, organisation, and analysis of data to draw inferences from the samples and apply them to the whole population. Variability is inherent in a population, process, or phenomenon. Statisticians value this variation and use it efficiently to make inferences or predictions about the population.\nMake stronger and smarter decisions faster\nAt ISCON Statistics, we offer robust statistical analysis and data modelling services. We use a cutting-edge combination of advanced statistical techniques and our expertise to develop analytical models for both conventional and big data.\nOur professional statisticians have hands-on experience in performing statistical analysis and modelling for a wide variety of data and research. We have developed our unique methodologies to expertly tackle a particular type of data and identify its characteristics.\nThrough our expertise, we can identify recurring patterns and trends, develop metrics and provide you with valuable data insights. This delivers optimal value and solutions for your research problems and business.\n ISCON Statistics has statistical consultants that are well equipped to handle all your statistical needs. We use various software packages, such as STATA, SPSS, and SAS, as one of the components of our data analysis services.\n Statistical analysis, statistical methods and statistical model Statistical analysis consists of sets of procedures to answer research questions which are - data collection (obtaining sample data from a population through different sampling methods), data description (describes the nature of sample data in the context of its distribution), data analysis with the use of statistical methods (which are well-defined, systematic, mathematical formulation and procedures), and interpretation of the results. Statistical modelling is the process of applying statistical methods to a dataset.\n ISCON Statistics has experienced statisticians who are familiar with various statistical methods and methods of qualitative analyses. With us, you will feel confident and assured in your ability to choose suitable statistical analyses for your data.\n Get powerful statistical analysis and modeling solutions As a leading analytical services provider, we offer powerful statistical analysis and modelling solutions. Our data models are readily applicable to social, industrial, geographical, experimental and financial data sets. We use the advanced tools and techniques for statistical analysis that include regression analysis, linear and non-linear regression analysis, time-series modelling, experimental and observational analysis, hypothesis testing and many others.\nWith our data modelling solutions, we provide you with valuable data insights. This helps you in improving the implications of your study outcomes and maximizes effectiveness.\nLet\u0026rsquo;s collaborate to help you overcome your data analysis and modelling challenges!\nDescriptive statistics and inferential statistics Descriptive statistics and inferential statistics are the two main branches of statistical science. Before we go into details about the difference between two, let us explain some basic concepts (with examples below), which will help you understand statistical analysis in more detail.\n Experiment: A scientific procedure undertaken to make a discovery or test a hypothesis Sampling Method: Random sampling from a population to ensure the validity of inference and conclusion Sample Data: A record of the observed quantity of variables of interest in a sample population Sample Statistic: The quantity of interest– mean, median, and the standard deviation in sample data Sampling Distribution Of Sample Statistic: Distribution of sample statistic on hypothetical random sampling Population Parameter: The true value of the variable interest in population which is unknown Standard Error: Variability of sample statistic on repeated sampling - standard deviation of sample statistic in the sampling distribution Confidence Interval: A statement about the true value of the population parameter lies in some interval with some degree of confidence  Lets take an example to understand above concepts. We want to know the average weight of the babies born in the UK (experiment); we cannot go to each hospital in the UK and weigh each baby (population). We need to choose a few which are representative of the population. Therefore, we need to have a list of all of the hospitals in the UK (sampling frame) and choose a few of them randomly (simple random sampling) and weigh each baby being born in the hospital (sample population). We will note down each baby’s weight (sample data) and calculate the mean (sample statistic). We will go to other hospitals, record the data, and calculate the mean. We will continue this process until we get the data from 100 hospitals. If we have sample data from 100 hospitals, we have a total 100 mean of the baby weight. However, each mean will not be the same because of random variation (sampling variability). If we plot the 100 mean of sample data, there will be some form of distribution (sampling distribution of sample statistic). The standard deviation of sample means in this distribution will be the standard error (standard error). If we truly had a random sample, the mean of the sampling distribution will be the true mean (population parameter), which is the average weight of the baby in the UK.\nDescriptive Statistics Descriptive statistics is concerned with describing the nature of observed data by some quantity that summarizes the data. These quantities can be measures of central tendency (location) and measures of variability (spread). Measures of central tendency are the mean, median, and mode, and measures of variability, such as the standard deviation, variance, the maximum and minimum values of the variable, and the skewness and kurtosis.\n ISCON Statistician can help you get the best results from your descriptive statistics projects by interpreting the findings of your research as well as presenting your statistics.\n Inferential statistics Inferential statistics is concerned with understanding something that is unobserved in the wider population. Statistical inference aims to estimate the uncertainty in hypothetical repeated sampling. This uncertainty will allow us to provide a plausible range of values for the true value of something in the population, such as the mean, and it allows us to make statements about plausibility in terms of the degree of confidence. All statistical inferences involve some form of a statistical model.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/statistical-analysis-modelling/","tags":null,"title":"Statistical Analysis \u0026 Modelling"},{"categories":null,"contents":"Why You Need Professional Dissertation Statistical Consulting Services As a post-graduate or PhD student, you must do research related to your field of study. You are required to perform a statistical analysis of your research study as its necessary part of obtaining results. Probably, you are good at doing research and authoring dissertation, but when it comes to statistical analysis, you may not have the sound knowledge to handle this part. It is because the statistical analysis is a complex process, which requires expertise. If you are facing trouble in handling this part of your research, you can take professional dissertation statistics help.\nProfessional Dissertation Statistical Consulting For Your Success in Academia ISCON Statistics provide expert dissertation statistical consulting services to help you with statistical data analysis. Our qualified and experienced team of statisticians has hands-on experience in dealing with all types of dissertation statistical analysis. We can efficiently perform both qualitative and quantitative statistical analysis of your data.\n Whether you need bivariate, univariate, cluster, factor or multivariate statistical analysis, we have you covered for all type of dissertation statistical analysis needs. Using the latest statistical tools and techniques such as SPSS, Matlab, STATA, R, SAS and others, we give you accurate and reliable results.\n With our professional help, you can complete and submit your dissertation within the prescribed deadline and according to the research standards of your particular subject.\nGet in touch with us now to have quick, accurate and reliable dissertation statistical analysis!\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/statistics-help-thesis/","tags":null,"title":"Dissertation statistical consulting"},{"categories":null,"contents":"Survey design and analysis is a science. Let ISCON Statistical’s statisticians design the questionnaire to get you the crucial data that you need to make critical decisions.\nISCON Statistical Services Ltd understands the importance of assessing the thoughts and opinions of your target audience. Our statisticians can help administer as well as analyze responses through various means, such as questionnaires, social media listening, and market research. With survey design and analysis services, we can help you in designing suitable questionnaires that increase response rates, minimize biases and errors, and improve the decision-making process.\nOur statistical consultants will take it all on ourselves to help you identify the question design, sampling method, and survey result analysis. Our professionals will translate your research goals and aim into an analysis plan based upon sound and reliable statistical practices.\nISCON Statistical Services Ltd has a unique and different way of looking at your customers, employees, and competitor’s satisfaction. We will leverage all publicly available data through various social channels to augment conventional survey research results. Our comprehensive service spectrum includes the following:\n Identifying your research objectives and the target market Giving a meaningful and suitable title to your survey questionnaire Ensuring correct layout and order Incorporating simple and interactive graphics to engage your audience Presenting the survey questionnaire in the right format to enable easy statistical analyses of data  You can rely on our expertise and industry experience to make this your best survey project, whether this is your first or latest online survey. Let ISCON Statistical provide survey services and correlate survey results so that you can concentrate on your business.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/survey-design-analysis/","tags":null,"title":"Survey Design \u0026 Analysis"},{"categories":null,"contents":"ISCON Statistics provides you with a wide range of high quality and reliable clinical research services. Our team of statisticians and data scientists offers development, consulting, and analysis services to organizations in pharmaceuticals, nutraceuticals, biotechnology, medical devices, and food and beverage industry.\nWe understand that the design of any clinical trial is an essential step in implementing an effective clinical research program. This is why investing in clinical trial design and analysis enables you to make the most of your investment. ISCON Statistics has several years of experience and will help to provide tailored and reliable clinical trial design specific to your specific research goals and needs.\nYou would like clinical trial design and protocols that can balance the interests of various stakeholders without breaking the bank. Our statistician will improve your study design, combining expertise and speed to deliver the best results. Our team of statisticians can recommend suitable trial designs that range from simple single endpoint designs to more advanced Bayesian adaptive designs.\nMost clinical trials and studies are often designed for the accurate evaluation of the safety, efficacy, or the mechanism of action of a new medical drug or device, medical procedure or product. ISCON Statistics offers support for design and analysis of clinical trials, observational studies, and experiments tailored to your specific needs and requirements.\nClinical Trial Design and Analysis Services:\n Selection of suitable patient population Stratification based on various biomarkers and prognostic features Literature review Regulatory counsel Treatment allocation methods, such as balanced and adaptive Choice of reliable and efficient endpoints Sample size calculation  Our multi-disciplinary environment will considerably add value to your research capabilities, better enabling you to design and launch an effective clinical trial to meet your goals.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/clinical-trials-design-analysis/","tags":null,"title":"Clinical Trials Design \u0026 Analysis"},{"categories":null,"contents":"With our epidemiology and genetic research services, we help you understand patient populations. Our clients trust our results, and this is where ISCON Statistics excels.\nEpidemiology helps with quantifying, segmenting, and projecting patient populations in the pharmaceutical industry. At ISCON Statistics, our team of statisticians will help you determine these populations through means, such as:\n A meta-analysis of disease statistics Global secondary research of reliable epidemiological literature Analyzing and deciphering patient registries and databases Assessing and analyzing disease trends, like changes in demographics, causational factors/diseases, and diagnostic criteria  We are at the forefront of modern epidemiologic methodology as well as practice. Driven by your research objectives, our statisticians bring scientific integrity, rigor to protocol design, research reporting, and data analytics. Their expertise elevates understanding and knowledge of the safety and efficacy of drugs, biologics, devices, and healthcare delivery.\nWe also offer affordable, cutting-edge genomic data management and analysis. ISCON Statistics has a strong and reliable history of excellence and a remarkable future in innovative genetics research.\nLeveraging the latest technology and resources, experts at ISCON Statistics have the capability and experience to undertake a wide array of research projects. These projects include large-scale and high throughput genome-wide association research studies on tens of thousands of samples as well as small-scale pilot candidate gene projects.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/epidemiology-genetics/","tags":null,"title":"Epidemiology \u0026 Genetics"},{"categories":null,"contents":"Get advice, training, and help from specialists and experts with in-depth knowledge and expertise of statistical analysis\nISCON Statistics provides you with on-site courses in Statistical Methods, Quality, and Reliability so that your team can turn the data you have into a valuable and reliable source of information to enhance and expand your business.\nOur statistical training programs are for clients in diverse industries and areas, such as clinical research and medical professionals, pharmaceutical and biotech companies, educational institutes, and regulatory agencies. Experienced consultants teach these courses, and they have the right blend of technical and practical experience. This ensures that your staff quickly gains the skills and knowledge they need.\nWe offer tailored training in statistical methods for each client with a singular focus on delivering stellar statistical knowledge so that clients can execute with more poise and confidence. The experience, education, communication skills, devotion, and passion that our instructors have provide an extraordinary learning experience for all our clients. Our training courses integrate the effective use of client data and applications in order to maximize relevancy and efficiency.\nOur training courses cover diverse topics, and some of them are:\n Root Cause Analysis Hypothesis Testing 3.Problem Solving Predictive Modeling Experimental Design and Analysis Statistical Analysis Warranty Forecasting Assessment of Measurement Systems  With long-standing experience and strict adherence to protocols and processes, ISCON Statistics has a reputation of being one of the most effective and easiest teams to work with.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/statistical-consulting-services/statistics-training/","tags":null,"title":"Statistics training"},{"categories":["Basic Statistics"],"contents":"\rThis article explains bootstrap concept as a whole and discern the fundamental difference between parametric and nonparametric bootstrap using R. We will be using both-for loops and standard boot package\nTable of Contents \n\rFundamental of statistical inference\rBootstrapping\rNonparametric bootstrap\rParametric Bootstrap\r\rFundamental of statistical inference\rWe first need to understand following some fundamental concepts. The idea behind bootstrapping is very closely aligned with those concepts.\nHere I have created a hypothetical study in which we are interested in finding out the average birth weight of the babies born in the UK at 37 weeks of gestation.\n\rExperiment : To find out the average weight of babies born at 37 weeks in the UK\n\rRandom sample : Random sample means each and every single individual or object has an equal probability of being chosen from the population. For example, in our study, the population is all birth happened in the UK at 37 weeks of gestation. For typical “random sample”, we will need to make sure that each and every birth happening in any corner of the UK, will be participating in our study, from which will choose some of them randomly i.e random sample.\n\rSample data: the recorded observation or quantity from the sample of the population. Now we have gone to one local hospital and got data of the birth-weights from 20 babies. Here is the (hypothetical) sample data.\n\r\r## [1] 3331.608 3549.913 2809.252 2671.465 3383.177 3945.721 3672.601 3922.416\r## [9] 4647.278 4088.246 3718.874 3724.443 3925.457 3112.920 3495.621 3651.779\r## [17] 3240.194 3867.347 3431.015 4163.725\r\rSample statistic : is the quantity of interest from sample data. The quantity of interest in our study is a mean (average). So mean is our sample statistic. So sample statistic in our sample data is 3618,\r\r## [1] 3617.653\rknitr::include_graphics(\u0026quot;/images/learnstatistics/boot.png\u0026quot;,error = FALSE)\r\rFigure 1: Parametric vs nonparametric bootstrap\r\r\rSampling distribution (of statistic) : Now we went to take a random sample of 20 babies at numerous hospitals (i.e.multiple times) in the UK. Each time, we calculate the mean birth weight and note down its value. This value will be different in each different hospital (in a statistical sense, the statistic is a random variable, will vary as variation is inherent). Here we went to 100 hospitals and took a sample of 20 babies born at that hospital and calculated the mean birth weight and plotted in the histogram.\r\rrequire(ggplot2)\rggplot(data.frame(statistic), aes(statistic)) + geom_histogram() + labs(x= \u0026quot;Mean\u0026quot;, y= \u0026quot;Freqency\u0026quot;)\r\rFigure 2: Distribution\r\rIt is better to plot the histogram with optimum bin width (see Article from David Freedman) along with density curve using aes(y=..density..).So that the area under the density curve will be 1\nbinw \u0026lt;- 2*(IQR(statistic)/length(statistic)^(1/3))\rggplot(data.frame(statistic), aes(statistic)) + geom_histogram(aes(y=..density..),binwidth = binw,fill=\u0026quot;darkred\u0026quot;,colour=\u0026quot;black\u0026quot;,size=1) + geom_density(aes(y=..density..),size=1) + labs(x= \u0026quot;Mean\u0026quot;, y= \u0026quot;Density\u0026quot;)\r\rFigure 3: Distribution of sample means (proper histogram)\r\r\rPopulation parameter : What will be the mean birth-weight of babies born in the UK if we had gone on collecting birthright of each and every single baby born in the UK. This will be only one value that what we are basically interested in. This is called the population parameter. The problem is that we want to estimate that from our limited number of observations (i.e sample data). The sample statistic will help to estimate the population parameter. The sample statistic will also be called point estimator.\n\rStandard error(of statistic) is the standard deviation (SD) of statistic from its sampling distribution. Note that SD is a statistic that measures the variability in data relative to its mean.\n\rConfidence interval : Though we are interested in population parameter i.e single value, what if our sample data may not contain it. We need to provide some measure of it. This can be achieved by building a confidence interval. We can say that the true value of the population lies in some interval with some degree of confidence.\n\r\rSo, in essence, we want to estimate a population parameter from the sampling distribution of the sample statistic. This sampling distribution can be generated by infinite time random sampling.\n\rBootstrapping\rThe bootstrap method is one type of re-sampling method, in which sample data (20 birth weights) considered as “population”.From this sample data, we re-sample it with a replacement-large number of time (the 1000’s). The resultant sampling distribution often (not always) approximate the (true) sampling distribution of the statistic.\nPlease note, that this sampling will be done with the replacement. So some single observation may be included many times ( or some may be left out completely). So sample statistic will be varied from each random sample of size n.\nFrom this bootstrapped sampling distribution, which can estimate parameter value, standard errors (standard deviation of sample statistic) and then, calculate the confidence interval.\nLet,\n\r\\(\\theta\\) is population parameter of interest which belongs to unknown population distribution F\n\r\\(\\hat{\\theta}\\) is statistic that estimate the \\(\\theta\\) and we are interests in sampling distribution of \\(\\hat{\\theta}\\) from fitted distribution function \\(\\hat{F}\\)\n\r\\(\\hat{\\theta_0^*}\\),\\(\\hat{\\theta_1^*}\\),\\(\\hat{\\theta_2^*}\\)… bootstrap estimate (statistic) to obtain sampling distribution of \\(\\hat{\\theta^*}\\) is \\(\\hat{F^*}\\). This sampling distribution is good approximation of \\(\\hat{F}\\)\n\r\r\rNonparametric bootstrap\rWe observed the sample data and we are unable to determine from which probability distribution function may have generated this sample data. In this situation, we use empirical distribution function (which is based on observed sample data) to simulate bootstrap samples (using Monte Carlo techniques).\n# loop\rtheta_star \u0026lt;- vector()\rfor (i in 1:1000){\rtheta_star[i] \u0026lt;- mean(sample(df,size=length(df),replace = T))\r# First take sample with replacement from obseerved data of same size \u0026amp; then, calculte sample statistic in each, repeat this 1000 times\r}\rtheta_boot \u0026lt;- mean(theta_star) # bootstrap estimate of theta_hat\rtheta_boot\r## [1] 3616.166\rboot_se \u0026lt;- sd(theta_star) # standard eorrs of the estimate\rboot_se\r## [1] 102.6719\rbias \u0026lt;- theta_boot - mean(df)\rbias\r## [1] -1.487018\rMSE \u0026lt;- mean((theta_boot- mean(df))^2)\rMSE\r## [1] 2.211222\rCI \u0026lt;-c(theta_boot-1.96*boot_se,theta_boot +1.96*boot_se)\rCI\r## [1] 3414.929 3817.402\rWe can use a boot function from the boot package in R.It requires a function to calculate sample statistic ( in its statistic argument). The function must include observed data as the first argument and indices ( or weight) in the second argument.\nrequire(boot)\rtheta_star_function \u0026lt;- function(x,i) mean(x[i])\r#\rB \u0026lt;- boot(data = df, statistic = theta_star_function, R=1000, stype = \u0026quot;i\u0026quot; )\rB\r## ## ORDINARY NONPARAMETRIC BOOTSTRAP\r## ## ## Call:\r## boot(data = df, statistic = theta_star_function, R = 1000, stype = \u0026quot;i\u0026quot;)\r## ## ## Bootstrap Statistics :\r## original bias std. error\r## t1* 3617.653 0.9443187 98.78257\rWe can used default plots to see whether bootstrap samples are correctly sampled.\nplot(B)\r\rFigure 4: Bootstrap sampling distribution of sample mean\r\rOr, one can use ggplot to get the same figure\nbinw \u0026lt;- 2*(IQR(B$t)/length(B$t)^(1/3))\rggplot(data.frame(B$t), aes(B.t)) + geom_histogram(fill=\u0026quot;grey\u0026quot;, colour=\u0026quot;black\u0026quot;, binwidth = binw) + geom_vline(xintercept = B$t0, linetype=\u0026quot;dashed\u0026quot;, size=1) + labs(x=\u0026quot;Bootstrap sample statistic\u0026quot;, title=\u0026quot;Bootstrap sampling distribution of sample mean\u0026quot;)\r\rFigure 5: Bootstrap sampling distribution of sample mean (ggplot2)\r\rWe can get confidence interval by,\nboot.ci(B)\r## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\r## Based on 1000 bootstrap replicates\r## ## CALL : ## boot.ci(boot.out = B)\r## ## Intervals : ## Level Normal Basic ## 95% (3423, 3810 ) (3421, 3819 ) ## ## Level Percentile BCa ## 95% (3416, 3814 ) (3407, 3804 ) ## Calculations and Intervals on Original Scale\r\rParametric Bootstrap\rOnce we have a sample data, we may think that the observed data may have come from some known probability distribution function ( i.e normal, gamma or poisson or any).\nFor example in our sample birth weights, we may assume that observed birth-weight comes from normal distribution with parameter \\(\\mu\\) = 3617.7 and \\(\\sigma\\) = 464.6 ( which is estimated from observed data). See the below figure,\nggplot(data.frame(df), aes(df)) + geom_density() + labs(x= \u0026quot;birth weight\u0026quot;)\r\rFigure 6: Distribution of observed sample birthweights\r\rSo, instead of using observed data ( as a non-parametric bootstrap), we can use normal distribution function with probable parameter estimates ( which most likely the maximum likelihood estimates) for bootstrap re-sampling.\nLet, first do it with help of for loops in R.\ntheta_star \u0026lt;- vector()\rfor (i in 1:1000){\rtheta_star[i] \u0026lt;- mean(rnorm(length(df),mean = 3617.7 ,sd = 464.6))\r}\rtheta_boot \u0026lt;- mean(theta_star) # bootstrap estimate of theta_hat\rtheta_boot\r## [1] 3613.883\rboot_se \u0026lt;- sd(theta_star) # standard eorrs of the estimate\rboot_se\r## [1] 101.4233\rbias \u0026lt;- theta_boot - mean(df)\rbias\r## [1] -3.769489\rMSE \u0026lt;- mean((theta_boot- mean(df))^2)\rMSE\r## [1] 14.20905\rCI \u0026lt;-c(theta_boot-1.96*boot_se,theta_boot +1.96*boot_se)\rCI\r## [1] 3415.094 3812.673\rNow, using boot function,\nFor parametric bootstrap, one has to specify a function in ran.gen arguments, which tell the boots how random sample will be generated ( I mean, from which distribution, parameters you want to generate re-sample). The first argument will be the observed data and the second argument will be parameter estimates.This function should give random samples according to your assumed distribution function.\ngen_function \u0026lt;- function(x,mle) {\rdata \u0026lt;- rnorm(length(x),mle[1],mle[2])\rreturn(data)}\r# function to calculate sample statistic theta_star_function \u0026lt;- function(x,i) mean(x[i])\rThe mle values ( of parameter estimates ) passed to random generator function.\nB \u0026lt;- boot(data = df, sim = \u0026quot;parametric\u0026quot;, ran.gen = gen_function, mle = c(3617.7,464.6), statistic = theta_star_function, R=1000)\rB\r## ## PARAMETRIC BOOTSTRAP\r## ## ## Call:\r## boot(data = df, statistic = theta_star_function, R = 1000, sim = \u0026quot;parametric\u0026quot;, ## ran.gen = gen_function, mle = c(3617.7, 464.6))\r## ## ## Bootstrap Statistics :\r## original bias std. error\r## t1* 3617.653 0.05774845 101.8872\rplot(B)\r\rFigure 7: Parametric bootstrap using boot package\r\rboot.ci(B,type = \u0026quot;perc\u0026quot;)\r## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\r## Based on 1000 bootstrap replicates\r## ## CALL : ## boot.ci(boot.out = B, type = \u0026quot;perc\u0026quot;)\r## ## Intervals : ## Level Percentile ## 95% (3413, 3813 ) ## Calculations and Intervals on Original Scale\r\r","permalink":"https://xenodochial-johnson-2c8705.netlify.app/learn-statistics/bootstrap-resampling-parametric-nonparametric-boot-how-to/","tags":["Parametric Bootstrap","Non-parametric Bootstrap"],"title":"Bootstrapping in Statistics : Difference between Parametric and Nonparametric Bootstrap method"},{"categories":null,"contents":"Overview NHS specialist consultant performed population-based case-control study in UK based hospital with aim to identify important risk factor in development of Asthma and to find factors that has excellent discriminative ability between cases and control. The consultant has very limited statistical knowledge, was looking for statistician that not only provide statistical results but also explain the results in non-technical language as he needs to present this finding in international conference.\nChallenges  Sample data has high proportion of missing observations. Missing observation pose significant challenge to validity and reliability of statistical findings The statistical findings should be interpretable to lay-audience The assumption of statistical model needs to be met in order to provide unbiased, valid conclusion  \rthis is title of figthis is caption\n\r\rStatistical approach ISCON statistician apply Poisson regression model (rather than logistic model for binary outcome) based on explanatory factors so that the findings can be presented as risk ratio rather than odds ratio (which is difficult to interpret). Our statistician build model in both, observed data and on data with multiple imputation of missing observations using MICE (multiple imputation with chain equations). To find predictive ability of the factors, ROC (Receiver Operating Curve) analysis has been performed.\nOutcome: The project completed on time with detailed report explaining statistical methods and findings in non-technical language. Multivariate model after missing data imputation (under MAR assumption of missingness) provided an evidence that BMI and FEV1 are important predictors of events of Asthma exacerbation. 1 unit increase in BMI found to be associated with 4% increased risk of Asthma exacerbation (95% CI 1%,8%, P 0.01) and patient with high FEV1 has lower risk of events. 1 unit increase in FEV1 values found to reduce the risk by 1% (95% CI 0.3,2%, P 0.007). None of the parameter (FEV1, FeNO, ACT, BMI and exposure to smoking) has excellent discriminative ability between those who has asthma exacerbation and who had not. ACT has found to have higher AUC compared to others though require more data to confirm the findings. DeLong, DeLong, and Clarke-Pearson (1988) test the the null hypothesis of equality of AUC for compared parameters. In given data, test did not provide any evidence to suggest that at least one parameter has different discrimating ability between those who has asthma exacerbation and who had not (Chi-Square 5.5 df 4, P-value 0.27).\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/case-studies/asthma-poisson/","tags":null,"title":"What are the risk factors for developing asthma? Which of those factors have high predictive ability? UK based case-control study"},{"categories":null,"contents":"Overview NHS specialist consultant performed population-based case-control study in UK based hospital with aim to identify important risk factor in development of Asthma and to find factors that has excellent discriminative ability between cases and control. The consultant has very limited statistical knowledge, was looking for statistician that not only provide statistical results but also explain the results in non-technical language as he needs to present this finding in international conference.\nChallenges  Sample data has high proportion of missing observations. Missing observation pose significant challenge to validity and reliability of statistical findings The statistical findings should be interpretable to lay-audience The assumption of statistical model needs to be met in order to provide unbiased, valid conclusion  Statistical approach ISCON statistician apply Poisson regression model (rather than logistic model for binary outcome) based on explanatory factors so that the findings can be presented as risk ratio rather than odds ratio (which is difficult to interpret). Our statistician build model in both, observed data and on data with multiple imputation of missing observations using MICE (multiple imputation with chain equations). To find predictive ability of the factors, ROC (Receiver Operating Curve) analysis has been performed.\nOutcome: The project completed on time with detailed report explaining statistical methods and findings in non-technical language. Multivariate model after missing data imputation (under MAR assumption of missingness) provided an evidence that BMI and FEV1 are important predictors of events of Asthma exacerbation. 1 unit increase in BMI found to be associated with 4% increased risk of Asthma exacerbation (95% CI 1%,8%, P 0.01) and patient with high FEV1 has lower risk of events. 1 unit increase in FEV1 values found to reduce the risk by 1% (95% CI 0.3,2%, P 0.007). None of the parameter (FEV1, FeNO, ACT, BMI and exposure to smoking) has excellent discriminative ability between those who has asthma exacerbation and who had not. ACT has found to have higher AUC compared to others though require more data to confirm the findings. DeLong, DeLong, and Clarke-Pearson (1988) test the the null hypothesis of equality of AUC for compared parameters. In given data, test did not provide any evidence to suggest that at least one parameter has different discrimating ability between those who has asthma exacerbation and who had not (Chi-Square 5.5 df 4, P-value 0.27).\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/case-studies/ultra-roc/","tags":null,"title":"Case Study: How many reading I need in Ultrasonography"},{"categories":null,"contents":"Overview NHS specialist consultant performed population-based case-control study in UK based hospital with aim to identify important risk factor in development of Asthma and to find factors that has excellent discriminative ability between cases and control. The consultant has very limited statistical knowledge, was looking for statistician that not only provide statistical results but also explain the results in non-technical language as he needs to present this finding in international conference.\nChallenges  Sample data has high proportion of missing observations. Missing observation pose significant challenge to validity and reliability of statistical findings The statistical findings should be interpretable to lay-audience The assumption of statistical model needs to be met in order to provide unbiased, valid conclusion  Statistical approach ISCON statistician apply Poisson regression model (rather than logistic model for binary outcome) based on explanatory factors so that the findings can be presented as risk ratio rather than odds ratio (which is difficult to interpret). Our statistician build model in both, observed data and on data with multiple imputation of missing observations using MICE (multiple imputation with chain equations). To find predictive ability of the factors, ROC (Receiver Operating Curve) analysis has been performed.\nOutcome: The project completed on time with detailed report explaining statistical methods and findings in non-technical language. Multivariate model after missing data imputation (under MAR assumption of missingness) provided an evidence that BMI and FEV1 are important predictors of events of Asthma exacerbation. 1 unit increase in BMI found to be associated with 4% increased risk of Asthma exacerbation (95% CI 1%,8%, P 0.01) and patient with high FEV1 has lower risk of events. 1 unit increase in FEV1 values found to reduce the risk by 1% (95% CI 0.3,2%, P 0.007). None of the parameter (FEV1, FeNO, ACT, BMI and exposure to smoking) has excellent discriminative ability between those who has asthma exacerbation and who had not. ACT has found to have higher AUC compared to others though require more data to confirm the findings. DeLong, DeLong, and Clarke-Pearson (1988) test the the null hypothesis of equality of AUC for compared parameters. In given data, test did not provide any evidence to suggest that at least one parameter has different discrimating ability between those who has asthma exacerbation and who had not (Chi-Square 5.5 df 4, P-value 0.27).\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/case-studies/sample-size-now/","tags":null,"title":"Case Study: How many sample we need for feasibility study"},{"categories":["Multivariate analysis"],"contents":"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex. Commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt.\n Excepteur sint occaecat cupidatat non proident sunt culpa officia deserunt mollit anim id est laborum.Sed ut perspiciatis unde omnis.\n  Business Services Audit \u0026amp; Assurance IT Control Solutions  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex. Commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt.\n","permalink":"https://xenodochial-johnson-2c8705.netlify.app/learn-statistics/new-hugo-post/","tags":["regression"],"title":"New Hugo Post"},{"categories":["Data Management"],"contents":"\rReshaping though frequently required in data analysis, so often it remains confusing even if you are frequent user of reshape function. Here I have provided an simple example to elaborate more on each argument of reshape.\nTable of Contents \n\rReshape data from wide to long\rReshape data from long to wide\r\rReshape data from wide to long\rYour data in wide form if the multiple observations of item, place or person (i.e. units) has been recorded in single row (but in multiple column). This multiple observations may be of repeated measure type (observation are made repeatedly at different time point) or multiple characteristics of some unit (eg. height, length and width of square).\nLet’s take an example of wide data of repeated measure type. Person A is visiting clinic every month for their blood pressure check, and nurse note down the value for each month in single row which belong to this specific person A. See example below,\n# wide data\rdf \u0026lt;- data.frame(matrix(data = NA, nrow = 3, ncol = 5, dimnames = list(NULL, paste0(c(\u0026quot;id\u0026quot;,\u0026quot;name\u0026quot;,\u0026quot;jan\u0026quot;,\u0026quot;feb\u0026quot;,\u0026quot;march\u0026quot;)))))\rdf[1,] \u0026lt;- c(1,\u0026quot;A\u0026quot;,123,120,125)\rdf[2,] \u0026lt;- c(2,\u0026quot;B\u0026quot;,140,150,155)\rdf[3,] \u0026lt;- c(3,\u0026quot;C\u0026quot;,96,86,97)\rOur “wide” data look like\nknitr::kable(df,format = \u0026quot;latex\u0026quot;, caption = \u0026quot;wide data\u0026quot;)\rWe want a data in which each new observation in new row but within same column i.e. long data. To achieve that we need to use standard reshape command in R. The reshape has following argument:\n\ridvar : unique identifier for person,place or object on which observations(measurements) are made at different time points or repeatedly. Example Case ID\n\rvarying : if observation for specific individuals are made at different time points, in which columns values are recorded i.e time-varying columns. Example Jan, Feb, March\n\rtimevar: what will be the name of column once the time-varying columns above has been staked in rows. Example Month\n\rtimes : what will be the values (of time) once the time-varying columns above has been staked in rows. Example Jan, Feb, March\n\rv.names: what will be the values (of observations) once the time-varying columns above has been staked in rows. Example BP\n\rdirection: data needs to converted from wide to long format.\n\r\rdf_long \u0026lt;- reshape(df,\ridvar = \u0026quot;id\u0026quot;, #[unique identifier for person,place or object on which observations(measurments) are made at different time points or repeatedly]\rvarying = c(\u0026quot;jan\u0026quot;,\u0026quot;feb\u0026quot;,\u0026quot;march\u0026quot;), # [if observation for specific individulas are made at different time points, in which columns values are recorded i.e time-varying columns ]\rtimevar = \u0026quot;month\u0026quot;, # [what will be the name of column once the timevarying columns above has been staked in rows]\rtimes = c(\u0026quot;jan\u0026quot;,\u0026quot;feb\u0026quot;,\u0026quot;march\u0026quot;),\r# [what will be the values (of time) once the timevarying columns above has been staked in rows]\rv.names = \u0026quot;BP\u0026quot;, # [what will be the values (of observations) once the timevarying columns above has been staked in rows]\rdirection = \u0026quot;long\u0026quot;) # [we want to convert wide df into long one])\rOur “long” data look like\ndf_long \u0026lt;- arrange(df_long, id)\rkable(df_long,format = \u0026quot;pandoc\u0026quot;, caption = \u0026quot;long data\u0026quot;)\r\rTable 1: long data\r\rid\rname\rmonth\rBP\r\r\r\r1\rA\rjan\r123\r\r1\rA\rfeb\r120\r\r1\rA\rmarch\r125\r\r2\rB\rjan\r140\r\r2\rB\rfeb\r150\r\r2\rB\rmarch\r155\r\r3\rC\rjan\r96\r\r3\rC\rfeb\r86\r\r3\rC\rmarch\r97\r\r\r\rSometime, not only one type of measurement (BP) but also other types (such as heart rate-HR) are measured and recorded row wise. For example,\n# wide data\rdf \u0026lt;- data.frame(matrix(data = NA, nrow = 3, ncol = 8, dimnames = list(NULL, paste0(c(\u0026quot;id\u0026quot;,\u0026quot;name\u0026quot;,\u0026quot;BP_jan\u0026quot;,\u0026quot;BP_feb\u0026quot;,\u0026quot;BP_march\u0026quot;,\u0026quot;HR_jan\u0026quot;,\u0026quot;HR_feb\u0026quot;,\u0026quot;HR_march\u0026quot;)))))\rdf[1,] \u0026lt;- c(1,\u0026quot;A\u0026quot;,123,120,125,72,70,71)\rdf[2,] \u0026lt;- c(2,\u0026quot;B\u0026quot;,140,150,155,85,82,86)\rdf[3,] \u0026lt;- c(3,\u0026quot;C\u0026quot;,96,86,97,65,52,59)\rkable(df,format = \u0026quot;pandoc\u0026quot;, caption = \u0026quot;wide data- multiple category\u0026quot;)\r\rTable 2: wide data- multiple category\r\rid\rname\rBP_jan\rBP_feb\rBP_march\rHR_jan\rHR_feb\rHR_march\r\r\r\r1\rA\r123\r120\r125\r72\r70\r71\r\r2\rB\r140\r150\r155\r85\r82\r86\r\r3\rC\r96\r86\r97\r65\r52\r59\r\r\r\rThis data can be converted into “long” by using list for group of time-varying columns for varying\ndf_long \u0026lt;- reshape(df,\ridvar = \u0026quot;id\u0026quot;, varying = list(c(\u0026quot;BP_jan\u0026quot;,\u0026quot;BP_feb\u0026quot;,\u0026quot;BP_march\u0026quot;),c(\u0026quot;HR_jan\u0026quot;,\u0026quot;HR_feb\u0026quot;,\u0026quot;HR_march\u0026quot;) ), timevar = \u0026quot;month\u0026quot;, times = c(\u0026quot;jan\u0026quot;,\u0026quot;feb\u0026quot;,\u0026quot;march\u0026quot;),\rv.names = c(\u0026quot;BP\u0026quot;,\u0026quot;HR\u0026quot;), direction = \u0026quot;long\u0026quot;) \rdf_long \u0026lt;- arrange(df_long, id)\rkable(df_long,format = \u0026quot;pandoc\u0026quot;, caption = \u0026quot;long data\u0026quot;)\r\rTable 3: long data\r\rid\rname\rmonth\rBP\rHR\r\r\r\r1\rA\rjan\r123\r72\r\r1\rA\rfeb\r120\r70\r\r1\rA\rmarch\r125\r71\r\r2\rB\rjan\r140\r85\r\r2\rB\rfeb\r150\r82\r\r2\rB\rmarch\r155\r86\r\r3\rC\rjan\r96\r65\r\r3\rC\rfeb\r86\r52\r\r3\rC\rmarch\r97\r59\r\r\r\r\rReshape data from long to wide\rTo make data “wide” from long, the reshape function will need only two main arguments\n\ridvar: unique identifier of unit on which measurement are made\n\rtimevar : which column represent the timing of the observations ( so that reshape function associate it with the value for given time for each ID )\n\r\rIf you do not specify above two arguments, function will drop an error-\nError in [.data.frame (data, , idvar) : undefined columns selected\nIf you read above error carefully, it already specifying which arguments were missing. Here in above case missing argument was idvar.\nYou can optionally provide,\n\rv.names : which column represent values of the observations in long data (so that reshape function can transform these values into rows for each ID)\n\rsep: column names in wide format are going to be created using value of timesvar and integers. Specify how both will be seperated in column names.\n\r\rHere is the example\ndf_wide \u0026lt;- reshape(df_long,\ridvar = \u0026quot;id\u0026quot;,\r# unique identifier\rtimevar = \u0026quot;month\u0026quot;,\r# the column represent the timing of the observations\rv.names = c(\u0026quot;BP\u0026quot;,\u0026quot;HR\u0026quot;),\r# the columns represent the value of the observation (BP,HR)\rdirection = \u0026quot;wide\u0026quot;,\rsep = \u0026quot;_\u0026quot;\r)\rHere is the wide data\n\rTable 4: wide data\r\r\rid\rname\rBP_jan\rHR_jan\rBP_feb\rHR_feb\rBP_march\rHR_march\r\r\r\r1\r1\rA\r123\r72\r120\r70\r125\r71\r\r4\r2\rB\r140\r85\r150\r82\r155\r86\r\r7\r3\rC\r96\r65\r86\r52\r97\r59\r\r\r\r\r","permalink":"https://xenodochial-johnson-2c8705.netlify.app/learn-statistics/reshape-wide-long-data-r-how-to/","tags":["reshape"],"title":"Reshape data from wide to long or from long to wide in R"}]